{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import sys\n",
    "import pyspark as ps\n",
    "import warnings\n",
    "import re\n",
    "import json\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.context import SQLContext\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "from datetime import datetime\n",
    "import pyspark.sql.types as T \n",
    "from pyspark.sql.functions import split, explode\n",
    "\n",
    "#To get spark. working without throwing a NameError\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark # Call this only after findspark\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleInput = spark.createDataFrame(\n",
    "    [\n",
    "        (11, 1, \"1\"), # create your data here, be consistent in the types.\n",
    "        (11, 2, \"1\"),\n",
    "        (11, 2, \"1\"),\n",
    "        (11, 2, \"1\"),\n",
    "        (11, 2, \"1\"),\n",
    "        (11, 2, \"1\"),\n",
    "        (11, 2, \"2\"),\n",
    "        (11, 2, \"2\"),\n",
    "        (11, 2, \"2\"),\n",
    "        (11, 2, \"2\"),\n",
    "        (11, 2, \"2\"),\n",
    "        (11, 2, \"2\"),\n",
    "        (11, 2, \"2\"),\n",
    "        (11, 2, \"2\"),\n",
    "        (22, 2, \"1\"),\n",
    "        (22, 2, \"1\"),\n",
    "        (22, 2, \"2\"),\n",
    "        (22, 2, \"2\"),\n",
    "        (22, 2, \"2\"),\n",
    "    ],\n",
    "    ['Device ID', 'Content ID', 'Time Period'] # add your columns label here\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----------+\n",
      "|Device ID|Content ID|Time Period|\n",
      "+---------+----------+-----------+\n",
      "|       11|         1|          1|\n",
      "|       11|         2|          1|\n",
      "|       11|         2|          1|\n",
      "|       11|         2|          1|\n",
      "|       11|         2|          1|\n",
      "|       11|         2|          1|\n",
      "|       11|         2|          2|\n",
      "|       11|         2|          2|\n",
      "|       11|         2|          2|\n",
      "|       11|         2|          2|\n",
      "|       11|         2|          2|\n",
      "|       11|         2|          2|\n",
      "|       11|         2|          2|\n",
      "|       11|         2|          2|\n",
      "|       22|         2|          1|\n",
      "|       22|         2|          1|\n",
      "|       22|         2|          2|\n",
      "|       22|         2|          2|\n",
      "|       22|         2|          2|\n",
      "+---------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sampleInput.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ACTUAL FUNCTION\n",
    "\n",
    "#for renaming the columns\n",
    "from functools import reduce\n",
    "\n",
    "#allow us to use SQL Count function for aggregation in groupBY\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "#allow us to read csv to dataframe\n",
    "import pandas as pd #need Pandas\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "#eventClassifierDF can be a dataframe or the directory of a CSV file\n",
    "def usageCounter(eventClassifierDF, toCSV = False):\n",
    "    #event Classifier DF has features Device ID, Content ID(ID of song, story, etc.), Time Period\n",
    "    #Columns must be in order specified above\n",
    "    \n",
    "    #if CSV directory is inputted, otherwise assumes a Dataframe was inputted\n",
    "    if (type(eventClassifierDF) == str):\n",
    "        schema = StructType([StructField(\"Device ID\", IntegerType(), True),StructField(\"Content ID\", IntegerType(), True),\n",
    "                            StructField(\"Time Period\", StringType(), True)])\n",
    "        pd_df = pd.read_csv(eventClassifierDF)\n",
    "        pd_df.columns = [\"Extra\", \"Device ID\", \"Time Period\", \"Activity Count\"]\n",
    "        pd_df = pd_df.drop([\"Extra\"], axis = 1) #has an extra column for some reason, getting rid of it\n",
    "        eventClassifierDF = spark.createDataFrame(pd_df, schema=schema)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #make sure columns have correct names\n",
    "    oldColumns = eventClassifierDF.schema.names\n",
    "    df = eventClassifierDF.withColumnRenamed(\n",
    "        oldColumns[0], \"Device ID\").withColumnRenamed( #device id\n",
    "        oldColumns[1], \"Content ID\").withColumnRenamed( #content id\n",
    "        oldColumns[2], \"Time Period\") #time period\n",
    "    \n",
    "    if(csv): #if we want to write dataframe to CSV\n",
    "        df.toPandas().to_csv('Output.csv')\n",
    "    else: #if we want to return a dataframe\n",
    "        return df.groupBy(\"Device ID\", \"Time Period\").agg(count(\"*\")).withColumnRenamed(\"count(1)\", \"Activity Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+--------------+\n",
      "|Device ID|Time Period|Activity Count|\n",
      "+---------+-----------+--------------+\n",
      "|       11|          2|             8|\n",
      "|       11|          1|             6|\n",
      "|       22|          2|             3|\n",
      "|       22|          1|             2|\n",
      "+---------+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "usageCounter(sampleInput).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+--------------+\n",
      "|Device ID|Time Period|Activity Count|\n",
      "+---------+-----------+--------------+\n",
      "|       22|          1|             2|\n",
      "|       11|          2|             8|\n",
      "|       11|          1|             6|\n",
      "|       22|          2|             3|\n",
      "+---------+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "usageCounter(\"sampleInput.csv\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleInput.toPandas().to_csv('sampleInput.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "usageCounter(sampleInput).toPandas().to_csv('sampleOutput.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
